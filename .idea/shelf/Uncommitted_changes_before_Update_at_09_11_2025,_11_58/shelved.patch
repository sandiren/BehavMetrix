Index: migrations/versions/0a30a34ea46c_initial_tables.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/migrations/versions/0a30a34ea46c_initial_tables.py b/migrations/versions/0a30a34ea46c_initial_tables.py
new file mode 100644
--- /dev/null	(date 1762659174786)
+++ b/migrations/versions/0a30a34ea46c_initial_tables.py	(date 1762659174786)
@@ -0,0 +1,176 @@
+"""Initial tables
+
+Revision ID: 0a30a34ea46c
+Revises: 
+Create Date: 2025-11-09 07:32:54.777173
+
+"""
+from alembic import op
+import sqlalchemy as sa
+
+
+# revision identifiers, used by Alembic.
+revision = '0a30a34ea46c'
+down_revision = None
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    # ### commands auto generated by Alembic - please adjust! ###
+    op.create_table('behavior_definitions',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('code', sa.String(), nullable=False),
+    sa.Column('name', sa.String(), nullable=False),
+    sa.Column('description', sa.String(), nullable=True),
+    sa.Column('ontology_reference', sa.String(), nullable=True),
+    sa.PrimaryKeyConstraint('id'),
+    sa.UniqueConstraint('code')
+    )
+    op.create_table('data_ingestion_sessions',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('created_at', sa.DateTime(), nullable=False),
+    sa.Column('created_by', sa.String(), nullable=False),
+    sa.Column('source', sa.String(), nullable=False),
+    sa.Column('notes', sa.String(), nullable=True),
+    sa.PrimaryKeyConstraint('id')
+    )
+    op.create_table('enrichment_items',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('name', sa.String(), nullable=False),
+    sa.Column('description', sa.String(), nullable=True),
+    sa.Column('category', sa.String(), nullable=True),
+    sa.Column('success_indicator', sa.String(), nullable=True),
+    sa.PrimaryKeyConstraint('id'),
+    sa.UniqueConstraint('name')
+    )
+    op.create_table('observers',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('name', sa.String(), nullable=False),
+    sa.Column('affiliation', sa.String(), nullable=True),
+    sa.PrimaryKeyConstraint('id')
+    )
+    op.create_table('animals',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('persistent_id', sa.String(), nullable=False),
+    sa.Column('name', sa.String(), nullable=True),
+    sa.Column('cage_id', sa.String(), nullable=False),
+    sa.Column('sex', sa.String(), nullable=False),
+    sa.Column('age', sa.Integer(), nullable=True),
+    sa.Column('weight_kg', sa.Float(), nullable=True),
+    sa.Column('species', sa.String(), nullable=False),
+    sa.Column('matriline', sa.String(), nullable=True),
+    sa.Column('date_of_birth', sa.DateTime(), nullable=True),
+    sa.Column('photo_url', sa.String(), nullable=True),
+    sa.Column('ingestion_session_id', sa.Integer(), nullable=True),
+    sa.ForeignKeyConstraint(['ingestion_session_id'], ['data_ingestion_sessions.id'], ),
+    sa.PrimaryKeyConstraint('id')
+    )
+    with op.batch_alter_table('animals', schema=None) as batch_op:
+        batch_op.create_index(batch_op.f('ix_animals_cage_id'), ['cage_id'], unique=False)
+        batch_op.create_index(batch_op.f('ix_animals_persistent_id'), ['persistent_id'], unique=True)
+
+    op.create_table('behavior_logs',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('timestamp', sa.DateTime(), nullable=False),
+    sa.Column('context', sa.String(), nullable=True),
+    sa.Column('sample_type', sa.String(), nullable=False),
+    sa.Column('animal_id', sa.Integer(), nullable=False),
+    sa.Column('behavior_id', sa.Integer(), nullable=False),
+    sa.Column('observer_id', sa.Integer(), nullable=True),
+    sa.Column('interaction_partner_id', sa.Integer(), nullable=True),
+    sa.ForeignKeyConstraint(['animal_id'], ['animals.id'], ),
+    sa.ForeignKeyConstraint(['behavior_id'], ['behavior_definitions.id'], ),
+    sa.ForeignKeyConstraint(['interaction_partner_id'], ['animals.id'], ),
+    sa.ForeignKeyConstraint(['observer_id'], ['observers.id'], ),
+    sa.PrimaryKeyConstraint('id')
+    )
+    with op.batch_alter_table('behavior_logs', schema=None) as batch_op:
+        batch_op.create_index(batch_op.f('ix_behavior_logs_timestamp'), ['timestamp'], unique=False)
+
+    op.create_table('enrichment_logs',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('timestamp', sa.DateTime(), nullable=False),
+    sa.Column('duration_minutes', sa.Float(), nullable=True),
+    sa.Column('response', sa.String(), nullable=True),
+    sa.Column('notes', sa.String(), nullable=True),
+    sa.Column('tag', sa.String(), nullable=True),
+    sa.Column('animal_id', sa.Integer(), nullable=False),
+    sa.Column('enrichment_item_id', sa.Integer(), nullable=False),
+    sa.ForeignKeyConstraint(['animal_id'], ['animals.id'], ),
+    sa.ForeignKeyConstraint(['enrichment_item_id'], ['enrichment_items.id'], ),
+    sa.PrimaryKeyConstraint('id')
+    )
+    op.create_table('hormone_measurements',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('collected_at', sa.DateTime(), nullable=False),
+    sa.Column('hormone', sa.String(), nullable=False),
+    sa.Column('value', sa.Float(), nullable=False),
+    sa.Column('unit', sa.String(), nullable=False),
+    sa.Column('animal_id', sa.Integer(), nullable=False),
+    sa.ForeignKeyConstraint(['animal_id'], ['animals.id'], ),
+    sa.PrimaryKeyConstraint('id')
+    )
+    op.create_table('incident_observations',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('created_at', sa.DateTime(), nullable=False),
+    sa.Column('reason', sa.String(), nullable=False),
+    sa.Column('description', sa.String(), nullable=True),
+    sa.Column('attachment_url', sa.String(), nullable=True),
+    sa.Column('tags', sa.String(), nullable=True),
+    sa.Column('animal_id', sa.Integer(), nullable=True),
+    sa.ForeignKeyConstraint(['animal_id'], ['animals.id'], ),
+    sa.PrimaryKeyConstraint('id')
+    )
+    op.create_table('rank_scores',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('created_at', sa.DateTime(), nullable=False),
+    sa.Column('elo_score', sa.Float(), nullable=False),
+    sa.Column('davids_score', sa.Float(), nullable=True),
+    sa.Column('instability_flag', sa.Boolean(), nullable=False),
+    sa.Column('animal_id', sa.Integer(), nullable=False),
+    sa.ForeignKeyConstraint(['animal_id'], ['animals.id'], ),
+    sa.PrimaryKeyConstraint('id')
+    )
+    op.create_table('stress_logs',
+    sa.Column('id', sa.Integer(), nullable=False),
+    sa.Column('date', sa.DateTime(), nullable=False),
+    sa.Column('stress_score', sa.Integer(), nullable=False),
+    sa.Column('withdrawal', sa.Boolean(), nullable=False),
+    sa.Column('fear_grimace', sa.Boolean(), nullable=False),
+    sa.Column('self_biting', sa.Boolean(), nullable=False),
+    sa.Column('notes', sa.String(), nullable=True),
+    sa.Column('animal_id', sa.Integer(), nullable=False),
+    sa.ForeignKeyConstraint(['animal_id'], ['animals.id'], ),
+    sa.PrimaryKeyConstraint('id')
+    )
+    with op.batch_alter_table('stress_logs', schema=None) as batch_op:
+        batch_op.create_index(batch_op.f('ix_stress_logs_date'), ['date'], unique=False)
+
+    # ### end Alembic commands ###
+
+
+def downgrade():
+    # ### commands auto generated by Alembic - please adjust! ###
+    with op.batch_alter_table('stress_logs', schema=None) as batch_op:
+        batch_op.drop_index(batch_op.f('ix_stress_logs_date'))
+
+    op.drop_table('stress_logs')
+    op.drop_table('rank_scores')
+    op.drop_table('incident_observations')
+    op.drop_table('hormone_measurements')
+    op.drop_table('enrichment_logs')
+    with op.batch_alter_table('behavior_logs', schema=None) as batch_op:
+        batch_op.drop_index(batch_op.f('ix_behavior_logs_timestamp'))
+
+    op.drop_table('behavior_logs')
+    with op.batch_alter_table('animals', schema=None) as batch_op:
+        batch_op.drop_index(batch_op.f('ix_animals_persistent_id'))
+        batch_op.drop_index(batch_op.f('ix_animals_cage_id'))
+
+    op.drop_table('animals')
+    op.drop_table('observers')
+    op.drop_table('enrichment_items')
+    op.drop_table('data_ingestion_sessions')
+    op.drop_table('behavior_definitions')
+    # ### end Alembic commands ###
Index: migrations/env.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/migrations/env.py b/migrations/env.py
new file mode 100644
--- /dev/null	(date 1762659156881)
+++ b/migrations/env.py	(date 1762659156881)
@@ -0,0 +1,113 @@
+import logging
+from logging.config import fileConfig
+
+from flask import current_app
+
+from alembic import context
+
+# this is the Alembic Config object, which provides
+# access to the values within the .ini file in use.
+config = context.config
+
+# Interpret the config file for Python logging.
+# This line sets up loggers basically.
+fileConfig(config.config_file_name)
+logger = logging.getLogger('alembic.env')
+
+
+def get_engine():
+    try:
+        # this works with Flask-SQLAlchemy<3 and Alchemical
+        return current_app.extensions['migrate'].db.get_engine()
+    except (TypeError, AttributeError):
+        # this works with Flask-SQLAlchemy>=3
+        return current_app.extensions['migrate'].db.engine
+
+
+def get_engine_url():
+    try:
+        return get_engine().url.render_as_string(hide_password=False).replace(
+            '%', '%%')
+    except AttributeError:
+        return str(get_engine().url).replace('%', '%%')
+
+
+# add your model's MetaData object here
+# for 'autogenerate' support
+# from myapp import mymodel
+# target_metadata = mymodel.Base.metadata
+config.set_main_option('sqlalchemy.url', get_engine_url())
+target_db = current_app.extensions['migrate'].db
+
+# other values from the config, defined by the needs of env.py,
+# can be acquired:
+# my_important_option = config.get_main_option("my_important_option")
+# ... etc.
+
+
+def get_metadata():
+    if hasattr(target_db, 'metadatas'):
+        return target_db.metadatas[None]
+    return target_db.metadata
+
+
+def run_migrations_offline():
+    """Run migrations in 'offline' mode.
+
+    This configures the context with just a URL
+    and not an Engine, though an Engine is acceptable
+    here as well.  By skipping the Engine creation
+    we don't even need a DBAPI to be available.
+
+    Calls to context.execute() here emit the given string to the
+    script output.
+
+    """
+    url = config.get_main_option("sqlalchemy.url")
+    context.configure(
+        url=url, target_metadata=get_metadata(), literal_binds=True
+    )
+
+    with context.begin_transaction():
+        context.run_migrations()
+
+
+def run_migrations_online():
+    """Run migrations in 'online' mode.
+
+    In this scenario we need to create an Engine
+    and associate a connection with the context.
+
+    """
+
+    # this callback is used to prevent an auto-migration from being generated
+    # when there are no changes to the schema
+    # reference: http://alembic.zzzcomputing.com/en/latest/cookbook.html
+    def process_revision_directives(context, revision, directives):
+        if getattr(config.cmd_opts, 'autogenerate', False):
+            script = directives[0]
+            if script.upgrade_ops.is_empty():
+                directives[:] = []
+                logger.info('No changes in schema detected.')
+
+    conf_args = current_app.extensions['migrate'].configure_args
+    if conf_args.get("process_revision_directives") is None:
+        conf_args["process_revision_directives"] = process_revision_directives
+
+    connectable = get_engine()
+
+    with connectable.connect() as connection:
+        context.configure(
+            connection=connection,
+            target_metadata=get_metadata(),
+            **conf_args
+        )
+
+        with context.begin_transaction():
+            context.run_migrations()
+
+
+if context.is_offline_mode():
+    run_migrations_offline()
+else:
+    run_migrations_online()
